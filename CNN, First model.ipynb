{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN, First model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisasjoblom/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/lisasjoblom/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Flatten, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.callbacks import TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 199483 training samples, 49871 test samples\n",
      "The length of each review is 100.\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "with open('X_train_even.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "    \n",
    "with open('Y_train_even.pickle', 'rb') as handle:\n",
    "    Y_train = pickle.load(handle)\n",
    "    \n",
    "with open('X_test_even.pickle', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)\n",
    "    \n",
    "with open('Y_test_even.pickle', 'rb') as handle:\n",
    "    Y_test = pickle.load(handle)\n",
    "    \n",
    "# import dictionaries\n",
    "with open('ix_to_word.pickle', 'rb') as handle:\n",
    "    ix_to_word = pickle.load(handle)\n",
    "    \n",
    "# small change in format\n",
    "tmp = np.concatenate(X_train).ravel()\n",
    "X_train = np.reshape(tmp,(len(X_train),100))\n",
    "\n",
    "tmp = np.concatenate(X_test).ravel()\n",
    "X_test = np.reshape(tmp,(len(X_test),100))\n",
    "\n",
    "review_length = X_test.shape[1]\n",
    "num_words = 46210\n",
    "\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))\n",
    "print('The length of each review is {}.'.format(review_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 10000\n",
    "\n",
    "small_Y = Y_train[0:num_datapoints]\n",
    "small_X = X_train[0:num_datapoints]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_conv_model(review_length, num_words):\n",
    "    model = Sequential()\n",
    "    emb = Embedding(num_words, 200, input_length=review_length)\n",
    "    model.add(emb)\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_conv_model(review_length, num_words):\n",
    "    model = Sequential()\n",
    "    emb = Embedding(num_words, 200, input_length=review_length)\n",
    "    model.add(emb)\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_conv_regularized_model(review_length, num_words):\n",
    "    model = Sequential()\n",
    "    emb = Embedding(num_words, 200, input_length=review_length)\n",
    "    model.add(emb)\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_conv_regularized_larger_kernel_model(review_length, num_words):\n",
    "    model = Sequential()\n",
    "    emb = Embedding(num_words, 200, input_length=review_length)\n",
    "    model.add(emb)\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_conv_regularized_model(review_length, num_words):\n",
    "    model = Sequential()\n",
    "    emb = Embedding(num_words, 200, input_length=review_length)\n",
    "    model.add(emb)\n",
    "    model.add(Conv1D(filters=50, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=30, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=20, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_conv_more_filter_model(review_length, num_words):\n",
    "    model = Sequential()\n",
    "    emb = Embedding(num_words, 200, input_length=review_length)\n",
    "    model.add(emb)\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_conv_larger_kernel_model(review_length, num_words):\n",
    "    model = Sequential()\n",
    "    emb = Embedding(num_words, 200, input_length=review_length)\n",
    "    model.add(emb)\n",
    "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=100, kernel_size=5, padding='valid', activation='relu', strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_nn(review_length, num_words):\n",
    "    model = Sequential()\n",
    "    emb = Embedding(num_words, 200, input_length=review_length)\n",
    "    model.add(emb)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159586 samples, validate on 39897 samples\n",
      "Epoch 1/10\n",
      "159586/159586 [==============================] - 106s 666us/step - loss: 0.3165 - acc: 0.8630 - val_loss: 0.2748 - val_acc: 0.8875\n",
      "Epoch 2/10\n",
      "159586/159586 [==============================] - 74s 463us/step - loss: 0.2035 - acc: 0.9205 - val_loss: 0.2802 - val_acc: 0.8881\n",
      "Epoch 3/10\n",
      "159586/159586 [==============================] - 84s 526us/step - loss: 0.1284 - acc: 0.9527 - val_loss: 0.2946 - val_acc: 0.8931\n",
      "Epoch 4/10\n",
      "159586/159586 [==============================] - 88s 551us/step - loss: 0.0764 - acc: 0.9729 - val_loss: 0.3145 - val_acc: 0.8950\n",
      "Epoch 5/10\n",
      "159586/159586 [==============================] - 88s 552us/step - loss: 0.0468 - acc: 0.9834 - val_loss: 0.3951 - val_acc: 0.8923\n",
      "Epoch 6/10\n",
      "159586/159586 [==============================] - 88s 551us/step - loss: 0.0328 - acc: 0.9887 - val_loss: 0.4649 - val_acc: 0.8927\n",
      "Epoch 7/10\n",
      "159586/159586 [==============================] - 88s 552us/step - loss: 0.0245 - acc: 0.9914 - val_loss: 0.5680 - val_acc: 0.8895\n",
      "Epoch 8/10\n",
      "159586/159586 [==============================] - 87s 545us/step - loss: 0.0194 - acc: 0.9933 - val_loss: 0.5603 - val_acc: 0.8887\n",
      "Epoch 9/10\n",
      "159586/159586 [==============================] - 88s 551us/step - loss: 0.0153 - acc: 0.9947 - val_loss: 0.6208 - val_acc: 0.8865\n",
      "Epoch 10/10\n",
      "159586/159586 [==============================] - 88s 552us/step - loss: 0.0139 - acc: 0.9952 - val_loss: 0.6254 - val_acc: 0.8900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f65ff0b9ba8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = one_conv_model(review_length, num_words)\n",
    "tb = TensorBoard(log_dir='./logs/one_conv_model')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])\n",
    "#model.fit(small_X, small_Y, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159586 samples, validate on 39897 samples\n",
      "Epoch 1/10\n",
      "159586/159586 [==============================] - 24s 151us/step - loss: 0.3341 - acc: 0.8514 - val_loss: 0.2605 - val_acc: 0.8946\n",
      "Epoch 2/10\n",
      "159586/159586 [==============================] - 22s 136us/step - loss: 0.1930 - acc: 0.9267 - val_loss: 0.2478 - val_acc: 0.9039\n",
      "Epoch 3/10\n",
      "159586/159586 [==============================] - 22s 135us/step - loss: 0.1121 - acc: 0.9609 - val_loss: 0.2757 - val_acc: 0.9027\n",
      "Epoch 4/10\n",
      "159586/159586 [==============================] - 22s 139us/step - loss: 0.0630 - acc: 0.9792 - val_loss: 0.3466 - val_acc: 0.8986\n",
      "Epoch 5/10\n",
      "159586/159586 [==============================] - 22s 139us/step - loss: 0.0366 - acc: 0.9884 - val_loss: 0.4054 - val_acc: 0.8964\n",
      "Epoch 6/10\n",
      "159586/159586 [==============================] - 22s 140us/step - loss: 0.0275 - acc: 0.9908 - val_loss: 0.4519 - val_acc: 0.8974\n",
      "Epoch 7/10\n",
      "159586/159586 [==============================] - 22s 139us/step - loss: 0.0205 - acc: 0.9933 - val_loss: 0.5050 - val_acc: 0.8962\n",
      "Epoch 8/10\n",
      "159586/159586 [==============================] - 22s 139us/step - loss: 0.0178 - acc: 0.9940 - val_loss: 0.5304 - val_acc: 0.8957\n",
      "Epoch 9/10\n",
      "159586/159586 [==============================] - 22s 139us/step - loss: 0.0195 - acc: 0.9935 - val_loss: 0.5367 - val_acc: 0.8944\n",
      "Epoch 10/10\n",
      "159586/159586 [==============================] - 22s 140us/step - loss: 0.0142 - acc: 0.9954 - val_loss: 0.5638 - val_acc: 0.8941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6552697d30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = two_conv_model(review_length, num_words)\n",
    "tb = TensorBoard(log_dir='./logs/two_conv_model')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])\n",
    "#model.fit(small_X, small_Y, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159586 samples, validate on 39897 samples\n",
      "Epoch 1/10\n",
      "159586/159586 [==============================] - 29s 180us/step - loss: 0.3363 - acc: 0.8537 - val_loss: 0.2650 - val_acc: 0.8919\n",
      "Epoch 2/10\n",
      "159586/159586 [==============================] - 28s 174us/step - loss: 0.1921 - acc: 0.9260 - val_loss: 0.2633 - val_acc: 0.8976\n",
      "Epoch 3/10\n",
      "159586/159586 [==============================] - 28s 175us/step - loss: 0.1181 - acc: 0.9569 - val_loss: 0.3003 - val_acc: 0.8948\n",
      "Epoch 4/10\n",
      "159586/159586 [==============================] - 28s 175us/step - loss: 0.0736 - acc: 0.9736 - val_loss: 0.3469 - val_acc: 0.8967\n",
      "Epoch 5/10\n",
      "159586/159586 [==============================] - 28s 175us/step - loss: 0.0522 - acc: 0.9817 - val_loss: 0.3881 - val_acc: 0.8945\n",
      "Epoch 6/10\n",
      "159586/159586 [==============================] - 28s 175us/step - loss: 0.0396 - acc: 0.9860 - val_loss: 0.4463 - val_acc: 0.8922\n",
      "Epoch 7/10\n",
      "159586/159586 [==============================] - 28s 175us/step - loss: 0.0348 - acc: 0.9877 - val_loss: 0.4490 - val_acc: 0.8952\n",
      "Epoch 8/10\n",
      "159586/159586 [==============================] - 28s 175us/step - loss: 0.0315 - acc: 0.9887 - val_loss: 0.4690 - val_acc: 0.8959\n",
      "Epoch 9/10\n",
      "159586/159586 [==============================] - 28s 175us/step - loss: 0.0305 - acc: 0.9892 - val_loss: 0.4688 - val_acc: 0.8955\n",
      "Epoch 10/10\n",
      "159586/159586 [==============================] - 28s 175us/step - loss: 0.0216 - acc: 0.9926 - val_loss: 0.5118 - val_acc: 0.8928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6551a3e668>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = two_conv_regularized_model(review_length, num_words)\n",
    "tb = TensorBoard(log_dir='./logs/two_conv_regularized_model')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])\n",
    "#model.fit(small_X, small_Y, validation_split=0.2, epochs=10, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159586 samples, validate on 39897 samples\n",
      "Epoch 1/10\n",
      "159586/159586 [==============================] - 33s 204us/step - loss: 0.3303 - acc: 0.8561 - val_loss: 0.2616 - val_acc: 0.8950\n",
      "Epoch 2/10\n",
      "159586/159586 [==============================] - 31s 191us/step - loss: 0.1790 - acc: 0.9316 - val_loss: 0.2726 - val_acc: 0.8948\n",
      "Epoch 3/10\n",
      "159586/159586 [==============================] - 31s 192us/step - loss: 0.1000 - acc: 0.9639 - val_loss: 0.3138 - val_acc: 0.8922\n",
      "Epoch 4/10\n",
      "159586/159586 [==============================] - 31s 192us/step - loss: 0.0635 - acc: 0.9773 - val_loss: 0.3699 - val_acc: 0.8914\n",
      "Epoch 5/10\n",
      "159586/159586 [==============================] - 31s 191us/step - loss: 0.0454 - acc: 0.9841 - val_loss: 0.4105 - val_acc: 0.8937\n",
      "Epoch 6/10\n",
      "159586/159586 [==============================] - 31s 191us/step - loss: 0.0378 - acc: 0.9871 - val_loss: 0.4348 - val_acc: 0.8972\n",
      "Epoch 7/10\n",
      "159586/159586 [==============================] - 31s 192us/step - loss: 0.0342 - acc: 0.9880 - val_loss: 0.4473 - val_acc: 0.8959\n",
      "Epoch 8/10\n",
      "159586/159586 [==============================] - 31s 191us/step - loss: 0.0264 - acc: 0.9911 - val_loss: 0.5123 - val_acc: 0.8916\n",
      "Epoch 9/10\n",
      "159586/159586 [==============================] - 31s 192us/step - loss: 0.0247 - acc: 0.9915 - val_loss: 0.4927 - val_acc: 0.8945\n",
      "Epoch 10/10\n",
      "159586/159586 [==============================] - 30s 190us/step - loss: 0.0244 - acc: 0.9918 - val_loss: 0.5091 - val_acc: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f653b41d828>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = two_conv_regularized_larger_kernel_model(review_length, num_words)\n",
    "tb = TensorBoard(log_dir='./logs/two_conv_regularized_larger_kernel_model')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])\n",
    "#model.fit(small_X, small_Y, validation_split=0.2, epochs=10, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 100, 200)          9242000   \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 99, 50)            20050     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 99, 50)            200       \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 98, 30)            3030      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 98, 30)            120       \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 97, 20)            1220      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 97, 20)            80        \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_16 (Glo (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               5376      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 9,272,333\n",
      "Trainable params: 9,272,133\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Train on 159586 samples, validate on 39897 samples\n",
      "Epoch 1/10\n",
      "159586/159586 [==============================] - 25s 158us/step - loss: 0.3582 - acc: 0.8396 - val_loss: 0.2856 - val_acc: 0.8817\n",
      "Epoch 2/10\n",
      "159586/159586 [==============================] - 22s 138us/step - loss: 0.2234 - acc: 0.9125 - val_loss: 0.2709 - val_acc: 0.8925\n",
      "Epoch 3/10\n",
      "159586/159586 [==============================] - 22s 136us/step - loss: 0.1551 - acc: 0.9420 - val_loss: 0.2810 - val_acc: 0.8936\n",
      "Epoch 4/10\n",
      "159586/159586 [==============================] - 22s 137us/step - loss: 0.1081 - acc: 0.9613 - val_loss: 0.3216 - val_acc: 0.8912\n",
      "Epoch 5/10\n",
      "159586/159586 [==============================] - 22s 137us/step - loss: 0.0753 - acc: 0.9737 - val_loss: 0.3592 - val_acc: 0.8905\n",
      "Epoch 6/10\n",
      "159586/159586 [==============================] - 22s 135us/step - loss: 0.0581 - acc: 0.9798 - val_loss: 0.4073 - val_acc: 0.8888\n",
      "Epoch 7/10\n",
      "159586/159586 [==============================] - 22s 137us/step - loss: 0.0439 - acc: 0.9849 - val_loss: 0.4373 - val_acc: 0.8908\n",
      "Epoch 8/10\n",
      "159586/159586 [==============================] - 22s 137us/step - loss: 0.0366 - acc: 0.9870 - val_loss: 0.4775 - val_acc: 0.8905\n",
      "Epoch 9/10\n",
      "159586/159586 [==============================] - 22s 136us/step - loss: 0.0331 - acc: 0.9882 - val_loss: 0.5021 - val_acc: 0.8896\n",
      "Epoch 10/10\n",
      "159586/159586 [==============================] - 22s 137us/step - loss: 0.0289 - acc: 0.9899 - val_loss: 0.5182 - val_acc: 0.8880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6541f0ff60>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = three_conv_regularized_model(review_length, num_words)\n",
    "model.summary()\n",
    "tb = TensorBoard(log_dir='./logs/three_conv_regularized_model')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])\n",
    "#model.fit(small_X, small_Y, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159586 samples, validate on 39897 samples\n",
      "Epoch 1/10\n",
      "159586/159586 [==============================] - 38s 235us/step - loss: 0.3368 - acc: 0.8539 - val_loss: 0.2657 - val_acc: 0.8916\n",
      "Epoch 2/10\n",
      "159586/159586 [==============================] - 36s 223us/step - loss: 0.2055 - acc: 0.9198 - val_loss: 0.2573 - val_acc: 0.8971\n",
      "Epoch 3/10\n",
      "159586/159586 [==============================] - 36s 223us/step - loss: 0.1362 - acc: 0.9488 - val_loss: 0.2780 - val_acc: 0.8960\n",
      "Epoch 4/10\n",
      "159586/159586 [==============================] - 36s 223us/step - loss: 0.0909 - acc: 0.9670 - val_loss: 0.3208 - val_acc: 0.8955\n",
      "Epoch 5/10\n",
      "159586/159586 [==============================] - 35s 222us/step - loss: 0.0657 - acc: 0.9763 - val_loss: 0.3660 - val_acc: 0.8909\n",
      "Epoch 6/10\n",
      "159586/159586 [==============================] - 35s 222us/step - loss: 0.0499 - acc: 0.9819 - val_loss: 0.3839 - val_acc: 0.8932\n",
      "Epoch 7/10\n",
      "159586/159586 [==============================] - 35s 219us/step - loss: 0.0402 - acc: 0.9859 - val_loss: 0.4952 - val_acc: 0.8789\n",
      "Epoch 8/10\n",
      "159586/159586 [==============================] - 35s 220us/step - loss: 0.0360 - acc: 0.9869 - val_loss: 0.4404 - val_acc: 0.8950\n",
      "Epoch 9/10\n",
      "159586/159586 [==============================] - 36s 224us/step - loss: 0.0311 - acc: 0.9890 - val_loss: 0.4729 - val_acc: 0.8945\n",
      "Epoch 10/10\n",
      "159586/159586 [==============================] - 36s 223us/step - loss: 0.0282 - acc: 0.9902 - val_loss: 0.4757 - val_acc: 0.8959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6541038fd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = three_conv_more_filter_model(review_length, num_words)\n",
    "tb = TensorBoard(log_dir='./logs/three_conv_more_filter_model')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])\n",
    "#model.fit(small_X, small_Y, validation_split=0.2, epochs=10, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159586 samples, validate on 39897 samples\n",
      "Epoch 1/10\n",
      "159586/159586 [==============================] - 42s 262us/step - loss: 0.3382 - acc: 0.8510 - val_loss: 0.2690 - val_acc: 0.8897\n",
      "Epoch 2/10\n",
      "159586/159586 [==============================] - 39s 245us/step - loss: 0.1967 - acc: 0.9242 - val_loss: 0.2655 - val_acc: 0.8974\n",
      "Epoch 3/10\n",
      "159586/159586 [==============================] - 39s 246us/step - loss: 0.1268 - acc: 0.9531 - val_loss: 0.2818 - val_acc: 0.8979\n",
      "Epoch 4/10\n",
      "159586/159586 [==============================] - 39s 246us/step - loss: 0.0799 - acc: 0.9711 - val_loss: 0.3355 - val_acc: 0.8981\n",
      "Epoch 5/10\n",
      "159586/159586 [==============================] - 39s 246us/step - loss: 0.0574 - acc: 0.9797 - val_loss: 0.3840 - val_acc: 0.8895\n",
      "Epoch 6/10\n",
      "159586/159586 [==============================] - 39s 247us/step - loss: 0.0431 - acc: 0.9850 - val_loss: 0.4017 - val_acc: 0.8963\n",
      "Epoch 7/10\n",
      "159586/159586 [==============================] - 39s 247us/step - loss: 0.0344 - acc: 0.9880 - val_loss: 0.4457 - val_acc: 0.8942\n",
      "Epoch 8/10\n",
      "159586/159586 [==============================] - 39s 246us/step - loss: 0.0350 - acc: 0.9874 - val_loss: 0.4188 - val_acc: 0.8953\n",
      "Epoch 9/10\n",
      "159586/159586 [==============================] - 39s 246us/step - loss: 0.0295 - acc: 0.9896 - val_loss: 0.4691 - val_acc: 0.8987\n",
      "Epoch 10/10\n",
      "159586/159586 [==============================] - 39s 246us/step - loss: 0.0240 - acc: 0.9919 - val_loss: 0.4802 - val_acc: 0.8948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6541241da0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = three_conv_larger_kernel_model(review_length, num_words)\n",
    "tb = TensorBoard(log_dir='./logs/three_conv_larger_kernel_model')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])\n",
    "#model.fit(small_X, small_Y, validation_split=0.2, epochs=10, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159586 samples, validate on 39897 samples\n",
      "Epoch 1/10\n",
      "159586/159586 [==============================] - 19s 121us/step - loss: 0.3484 - acc: 0.8455 - val_loss: 0.2845 - val_acc: 0.8831\n",
      "Epoch 2/10\n",
      "159586/159586 [==============================] - 19s 118us/step - loss: 0.1256 - acc: 0.9545 - val_loss: 0.3635 - val_acc: 0.8730\n",
      "Epoch 3/10\n",
      "159586/159586 [==============================] - 19s 118us/step - loss: 0.0318 - acc: 0.9901 - val_loss: 0.5389 - val_acc: 0.8736\n",
      "Epoch 4/10\n",
      "159586/159586 [==============================] - 19s 118us/step - loss: 0.0098 - acc: 0.9972 - val_loss: 0.6924 - val_acc: 0.8731\n",
      "Epoch 5/10\n",
      "159586/159586 [==============================] - 19s 119us/step - loss: 0.0090 - acc: 0.9972 - val_loss: 0.7991 - val_acc: 0.8672\n",
      "Epoch 6/10\n",
      "159586/159586 [==============================] - 19s 119us/step - loss: 0.0222 - acc: 0.9924 - val_loss: 0.6884 - val_acc: 0.8674\n",
      "Epoch 7/10\n",
      "159586/159586 [==============================] - 19s 118us/step - loss: 0.0091 - acc: 0.9969 - val_loss: 0.8337 - val_acc: 0.8667\n",
      "Epoch 8/10\n",
      "159586/159586 [==============================] - 19s 118us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.9264 - val_acc: 0.8691\n",
      "Epoch 9/10\n",
      "159586/159586 [==============================] - 19s 118us/step - loss: 0.0067 - acc: 0.9977 - val_loss: 0.9760 - val_acc: 0.8652\n",
      "Epoch 10/10\n",
      "159586/159586 [==============================] - 19s 118us/step - loss: 0.0117 - acc: 0.9960 - val_loss: 0.9087 - val_acc: 0.8693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6551a18f98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vanilla_nn(review_length, num_words)\n",
    "tb = TensorBoard(log_dir='./logs/vanilla_nn_model')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])\n",
    "#model.fit(small_X, small_Y, validation_split=0.2, epochs=10, batch_size=256, verbose=1, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('two_conv_regularized_larger_kernel_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('two_conv_regularized_larger_kernel_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 100, 200)          9242000   \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 99, 100)           40100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 99, 100)           400       \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 96, 100)           40100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 96, 100)           400       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_19 (Glo (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 9,349,113\n",
      "Trainable params: 9,348,713\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "preds = np.round(model.predict(X_test)).T\n",
    "preds = preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 49871 test reviews, 5278 were misclassified.\n",
      "A few examples of the misclassified reviews:\n",
      "1. Predicted output: 1.0 Actual label: 0\n",
      "caution is in order if they're safe they deserve 5 stars because they're a special tasting and nearly pure treat if they're not safe they should be pulled off of the market there's a lot of google chatter about this product being toxic just look up kingdom pets chicken jerky it looks to me like only one or a few dogs got sick and those one or few incidents are being cited over and over on different sites making it seem like there's an epidemic if only a few got sick it might be from an unknown something else that\n",
      "2. Predicted output: 1.0 Actual label: 0\n",
      "i have to say that i really like the packaging the cereal name and part of the image have raised graphics that i felt as soon as i picked up the box however the cereal falls a little short of the cool packaging i have tried both the chocolate krave as shown here and the double chocolate krave the chocolate variety has a better flavor as voted by our household of 3 but that is not saying much you will have a more enjoyable cereal experience if the cereal is consumed straight from the box without milk once milk is\n",
      "3. Predicted output: 1.0 Actual label: 0\n",
      "they definitely came through on their promise to not have a single duplicate it is pretty awesome if you are a coffee snob and want to taste a veriety of different flavors i surely found what i did not like but my pallet is not refined enough to sort through so much variety i would recommend going with a smaller variety pack unless you just want to spend a lot ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO\n"
     ]
    }
   ],
   "source": [
    "misclassifications = np.where(preds!=Y_test)\n",
    "misclassifications = misclassifications[0]\n",
    "print('Out of {} test reviews, {} were misclassified.'.format(len(Y_test), len(misclassifications)))\n",
    "print('A few examples of the misclassified reviews:')\n",
    "\n",
    "\n",
    "def print_sentence(index):\n",
    "    sentence_ix = misclassifications[index]\n",
    "    tmp = []\n",
    "    for val in X_test[sentence_ix]:\n",
    "        tmp.append(ix_to_word[str(val)])\n",
    "    processed_sentence = ' '.join(tmp)\n",
    "    print(processed_sentence)\n",
    "  \n",
    "print('1. Predicted output: {} Actual label: {}'.format(preds[misclassifications[0]], Y_test[misclassifications[0]]))\n",
    "print_sentence(0)\n",
    "print('2. Predicted output: {} Actual label: {}'.format(preds[misclassifications[1]], Y_test[misclassifications[1]]))\n",
    "print_sentence(1)\n",
    "print('3. Predicted output: {} Actual label: {}'.format(preds[misclassifications[2]], Y_test[misclassifications[2]]))\n",
    "print_sentence(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
