{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from string import punctuation\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "#nltk.download('perluniprops')\n",
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Label</th>\n",
       "      <th>WordIndeces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 16, 123, 317, 7, 1, 4998, 516, 102, 51, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[40, 372, 2210, 22, 5809, 1960, 1075, 1, 1075,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[9, 8, 4, 7126, 13, 49, 82, 275, 4, 162, 9315,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[35, 18, 19, 250, 10, 1, 2625, 570, 11, 24049,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[37, 3580, 31, 4, 37, 86, 78, 20, 4, 2073, 214...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score  Label                                        WordIndeces\n",
       "0      5      1  [2, 16, 123, 317, 7, 1, 4998, 516, 102, 51, 20...\n",
       "1      1      0  [40, 372, 2210, 22, 5809, 1960, 1075, 1, 1075,...\n",
       "2      4      1  [9, 8, 4, 7126, 13, 49, 82, 275, 4, 162, 9315,...\n",
       "3      2      0  [35, 18, 19, 250, 10, 1, 2625, 570, 11, 24049,...\n",
       "4      5      1  [37, 3580, 31, 4, 37, 86, 78, 20, 4, 2073, 214..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tokenized_reviews.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as labels and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = df.WordIndeces.values\n",
    "labels = df.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 500, 1)\n"
     ]
    }
   ],
   "source": [
    "tmp = np.concatenate(input_array).ravel()\n",
    "input_data = np.reshape(tmp,(len(input_array),500,1))\n",
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Add your import statements here\n",
    "from keras import Input, Model\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def create_model(X_seq_len, output_n, hidden_dim, b_size):\n",
    "    \"\"\" Define a keras sequence-to-sequence model. \n",
    "    \n",
    "    Arguments:\n",
    "    input_n - integer, the number of inputs for the network (the length of a one-hot vector from `X`)\n",
    "    X_seq_len - integer, the length of a sequence from `X`. Should be constant and you made sure by using padding\n",
    "    output_n - integer, the number of outputs for the network (the length of a one-hot vector from `Y`)\n",
    "    Y_seq_len - integer, the length of a sequence from `Y`. Should be constant and you made sure by using padding\n",
    "    hidden_dim - integer, number of units in the LSTM's memory cell.\n",
    "    embedding_dim - output dimension of the embedding layer.\n",
    "    \n",
    "    Returns:\n",
    "    The compiled keras model\n",
    "    \n",
    "    \"\"\"\n",
    "    # Input and embedding layers\n",
    "    input_layer = Input(batch_shape=(b_size,X_seq_len),shape=(X_seq_len,1))\n",
    "    input_layer.reshape((b_size,X_seq_len,1))\n",
    "    print(input_layer)\n",
    "    \n",
    "    # Create the encoder LSTM.\n",
    "    # correct number of units?\n",
    "    encoder_LSTM = LSTM(units=hidden_dim, activation='tanh', recurrent_activation='hard_sigmoid')\n",
    "    encoder_output = encoder_LSTM(input_layer)\n",
    "    \n",
    "    # Add a fully connected layer and a softmax to the outputs of the decoder\n",
    "    fully_connected_output = TimeDistributed(Dense(output_n))(encoder_output)\n",
    "    softmax_output = Activation('softmax')(fully_connected_output)\n",
    "    \n",
    "    # Create final model and compile it\n",
    "    model = Model([input_layer], softmax_output)\n",
    "    \n",
    "    # Compile the model. Use a loss function, optimizer, and metrics of your choice\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Add these arguments to the model for convenience\n",
    "    model.hidden_dim = hidden_dim\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "def create_model(X_seq_len, output_n, hidden_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_dim, input_shape=(X_seq_len, 1)))\n",
    "    model.add(Dense(output_n,activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lessen the amount of data\n",
    "num_smaller = 10000\n",
    "input_data_lessen = input_data[0:num_smaller]\n",
    "labels_lessen = labels[0:num_smaller]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model, uneven split (bleh bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 210s 26ms/step - loss: 0.5872 - acc: 0.7608 - val_loss: 0.5694 - val_acc: 0.7440\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 219s 27ms/step - loss: 0.5445 - acc: 0.7660 - val_loss: 0.5699 - val_acc: 0.7440\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 209s 26ms/step - loss: 0.5446 - acc: 0.7660 - val_loss: 0.5716 - val_acc: 0.7440\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 207s 26ms/step - loss: 0.5449 - acc: 0.7660 - val_loss: 0.5690 - val_acc: 0.7440\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 212s 27ms/step - loss: 0.5444 - acc: 0.7660 - val_loss: 0.5692 - val_acc: 0.7440\n"
     ]
    }
   ],
   "source": [
    "X_seq_len = len(input_data[0]) #the input data is padded\n",
    "output_dim = 1\n",
    "hidden_dim = 10\n",
    "num_epochs = 5\n",
    "b_size = 64\n",
    "\n",
    "model = create_model(X_seq_len,output_dim,hidden_dim)\n",
    "name = 'first_model'\n",
    "\n",
    "# Define a tensorboard callback\n",
    "tb = TensorBoard(log_dir='./logs/'+name)\n",
    "    \n",
    "# Print model summary and train\n",
    "model.summary()\n",
    "current_history = model.fit(input_data_lessen, labels_lessen, verbose=1, batch_size=b_size, epochs=num_epochs, callbacks=[tb], validation_split=0.2);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch even data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of even training data points: 199483\n"
     ]
    }
   ],
   "source": [
    "with open('X_train_even.pickle', 'rb') as handle:\n",
    "    X_train_even = pickle.load(handle)\n",
    "    \n",
    "# with open('X_test_even.pickle', 'rb') as handle:\n",
    "#     X_test_even = pickle.load(handle)\n",
    "    \n",
    "with open('Y_train_even.pickle', 'rb') as handle:\n",
    "    Y_train_even = pickle.load(handle)\n",
    "    \n",
    "# with open('Y_test_even.pickle', 'rb') as handle:\n",
    "#     Y_test_even = pickle.load(handle)\n",
    "\n",
    "print('Total number of even training data points: %d' %len(X_train_even))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model on even data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a smaller data set for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The share of negative reviews:\n",
      "0.4924\n",
      "It is approximately even!\n"
     ]
    }
   ],
   "source": [
    "lesser_number = 10000\n",
    "\n",
    "small_Y = Y_train_even[0:lesser_number]\n",
    "small_X = X_train_even[0:lesser_number]\n",
    "neg_share = len(small_Y[small_Y==1])/lesser_number\n",
    "\n",
    "print('The share of negative reviews:')\n",
    "print(neg_share)\n",
    "print('It is approximately even!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 217s 27ms/step - loss: 0.6927 - acc: 0.5048 - val_loss: 0.6933 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 214s 27ms/step - loss: 0.6926 - acc: 0.5034 - val_loss: 0.6933 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 216s 27ms/step - loss: 0.6925 - acc: 0.5085 - val_loss: 0.6933 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 212s 26ms/step - loss: 0.6926 - acc: 0.5088 - val_loss: 0.6933 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 207s 26ms/step - loss: 0.6925 - acc: 0.5088 - val_loss: 0.6932 - val_acc: 0.5055\n"
     ]
    }
   ],
   "source": [
    "X_seq_len = len(small_X[0]) #the input data is padded\n",
    "output_dim = 1\n",
    "hidden_dim = 10\n",
    "num_epochs = 5\n",
    "b_size = 64\n",
    "\n",
    "model = create_model(X_seq_len,output_dim,hidden_dim)\n",
    "name = 'first_model_even_data'\n",
    "\n",
    "# Define a tensorboard callback\n",
    "tb = TensorBoard(log_dir='./logs/'+name)\n",
    "    \n",
    "# Print model summary and train\n",
    "model.summary()\n",
    "current_history = model.fit(small_X, small_Y, verbose=1, batch_size=b_size, epochs=num_epochs, callbacks=[tb], validation_split=0.2);\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
